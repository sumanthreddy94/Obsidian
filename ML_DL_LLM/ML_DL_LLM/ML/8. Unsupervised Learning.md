[Colab](https://colab.research.google.com/drive/1IEN9OQ-GdF52SF2V4_6G7xyKTjZ8KZNU?usp=sharing)
### K Means Clustering
Here we Compute *K Clusters from Data*

	We don't have  Y label for these data
	We plot Feature 1 vs Feature 2
	The resultant plot may have differentn number of clusters based on k
	k is predefined(Given)
	
1. Given k = 3, algorithm chooses 3 random locations for to be centroids
	2. Do **Epectation**: Calculate the distance b/w each point to each of centroid, That gives you way to assign each of the point to the nearest centroid ()
	3. **Expectation Maximization**: Re Compute new centroids (new locations), Step - 2 repeat (re iterate)
	4. Stop iterating when the data points stop changing from one centroid to another. Now we have nice clusters

### Principal Component Analysis
We do: *Dimensionality Reduction Technique*


Lets say we have X1, X2, X3....! Can we reduce all these dimensions into a single meaningful dimension, We call it Dimensionality Reduction Technique

let's say we have X0, X1 features space plot
Let's say X0  (X-axis)-> Years since house is built X1 (Y-axis) -> Square Footage

**Principal Component**: It is a direction in space with largest Variance

Map all these points to 1 - D  space so transformed Dataset is on the regression like line(PCA line). Map meaning, perpendicular Projection of points on to this line. 
We minimize the projection residual(distance between actual and projection data point)
1. Minimize Projection Residuals
2. Maximize Variance
We need linear algebra, Eigen values etc. to understand this

Now we have 1 - D input data.

We use it when have like 100s of relevant dimensions, we boil it to top 5 principal dimensions.








