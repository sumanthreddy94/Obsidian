[Colab](https://colab.research.google.com/drive/1FtYtO2CE771UrUbnrspY1WBTODSoWNP-?usp=sharing)

Dataset =>
Split the data => 70 & 30 => 
Train with 70 =>  ML => Trained Model
Test the trained with 30 => Predict and Evaluate

### Linear Regression Algorithm:
Supervised ML => Regression =>
Task of Regression => Predicting Real value of Y
Stats module, Psykit learn model

Only Numerical Values  Qualifies for Regression

***Derived Using Probability***
***Maximum Likelihood Estimation (MLE)***

y^ = a + bx
y^ = c + b1x1 + b2x2.....!  => x1,x2,x3 are features (Multiple features)
Output: Coefficients and Interceptors (i.e) c and b1, b2, b3...!
y^ is predicted value

### Evaluation:
1. Mean Absolute Error (MAE)
2. Mean Squared Error (MSE) => Mean of Squared(MAE)
3. Root Mean Squared Error (RMSE) => Root of (MSE)
4. R2 Score 
R2 Score is calculating how much variance(information) can be explained by using the given features(x1, x2, x3). 
If you can predict complete Y with X, we will have 1 which is 100% => Exactly on the regression line
Accuracy is where given features give how near are the prediction to regression line

### Regularization & Assumptions of LR
Need:
LR algo uses Loss Functions to provide better b1, b2,b3..! such that 
squared error is minimal. One such loss function is Squared Error Loss.
Even then We might not get better performance in SOME SCENARIOS
(Overfitting, more Variables,  memorizing data instead of generalization,
Not enough Data, Training too slow)

We need Regularization:
1. Lasso Regularization (L1):
	Add penalty term for Loss Function
	L = min(squared Error Loss) + lambda * || b||*
	 Tends to make predictions to Abs Zero
	 Because we are adding Abs Value of Magnitude of coefficient as penalty term to loss function
	
2. Ridge Regularization
3. Elastic Regression Applies L1 and L2 in ratio

- **Lasso**: Use when you want a simpler, interpretable model that automatically does **feature selection**.
- **Ridge**: Use when you believe **all features matter** but want to reduce overfitting, especially with **multicollinearity**.
- **Elastic Net**: Use when you want the **best of both worlds** — feature selection and handling correlated features gracefully

### Assumptions:


> [!Assumption Impact]
> When linear regression **assumptions are violated**, it can impact the **validity, reliability, and interpretation** of your model

These are calculated on relation between Features and Target, and Between Features
1. Linear Relationship between feature and Target , => Pairplot or graphs can help
2. Mean Residuals (Given Dataset) = 0 
```
residual = (Y_test - Y_pred_test)
np.mean(residual)
```
3. Normal Distribution of Error Terms
```
sns.displot(residual)
plt.show()
```
4. Multi Collinearity => vif (Variance Inflation Factor)
	# Check if we have correlation between features
	If we have Collinearity, Ability to generate prediction reduces Hmm..! Why and Cool
	Because model gets confused like which factor is influencing the target
	VIF_SCORE > 4 --> Multicollinearity in the Dataset
	VIF_SCORE < 4 --> No Multicollinearity in the Dataset


